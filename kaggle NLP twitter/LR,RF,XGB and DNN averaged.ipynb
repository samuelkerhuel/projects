{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,make_scorer\n\nPUNCT_TO_REMOVE = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import training data\ndf_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TEXT PREPROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text):\n    '''\n    input: string to be processed\n    output: preprocssed string\n    '''\n    text = text.lower() # make everything lower case\n    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', text) #remove url\n    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)) #remove punctuation\n    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS]) #remove stop words\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_preprocessing('#Flashflood causes #landslide in Gilgit #Pakistan Damage to 20 homes farmland roads and bridges #365disasters  http://t.co/911F3IXRH0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text_processed'] = df_train['text'].apply(text_preprocessing)\ndf_test['text_processed'] = df_test['text'].apply(text_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train['text_processed']\ny = df_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer=TfidfVectorizer(ngram_range=(1,3),min_df=3,strip_accents='unicode', \n                           use_idf=1,smooth_idf=1, sublinear_tf=1,max_features=None)\nvectorizer.fit(list(df_train['text_processed'])+list(df_test['text_processed']))\nprint('vocab length',len(vectorizer.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_onehot = vectorizer.transform(X_train).todense()\nX_val_onehot = vectorizer.transform(X_val).todense()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base Model: Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(max_iter=150,penalty='l2',solver='lbfgs',random_state=0)\nlr_clf.fit(X_train_onehot, y_train)\nlr_pred = lr_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(lr_pred,y_val))\nprint(classification_report(y_val, lr_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will try RF, XGBoost and DNN"},{"metadata":{},"cell_type":"markdown","source":"### RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state=0,n_estimators=100,\n                                max_depth=None, verbose=0,n_jobs=-1)\nrf_clf.fit(X_train_onehot, y_train)\nrf_pred = rf_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(rf_pred,y_val))\nprint(classification_report(y_val, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nxgb_clf = xgb.XGBClassifier(n_estimators=100,n_jobs=-1,max_depth=15,min_child_weight=3,objective='binary:logistic',colsample_bytree=0.4)\nxgb_clf.fit(X_train_onehot, y_train)\nxgb_predictions = xgb_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(xgb_predictions,y_val))\nprint(classification_report(y_val, xgb_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(X_train_onehot)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import regularizers\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=np.shape(X_train_onehot)[1],kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0001, amsgrad=False)\nmodel.compile(optimizer= adam,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nprint(model.summary())\n\n# Train the model, iterating on the data in batches of 32 samples\nhist = model.fit(X_train_onehot, y_train,validation_data = (X_val_onehot,y_val), epochs=20, batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nhistory = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12));\nplt.plot(history[\"loss\"]);\nplt.plot(history[\"val_loss\"]);\nplt.title(\"Loss as function of epoch\");\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_pred = model.predict_classes(X_val_onehot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy score: ',accuracy_score(dnn_pred,y_val))\nprint(classification_report(y_val, dnn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text_processed'] = df_test['text'].apply(text_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test['text_processed']\nX_test_onehot = vectorizer.transform(X_test).todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_predictions = lr_clf.predict_proba(X_test_onehot)\nrf_predictions = rf_clf.predict_proba(X_test_onehot)\nxgb_predictions = xgb_clf.predict_proba(X_test_onehot)\ndnn_predictions = model.predict_proba(X_test_onehot).ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = 1/4*lr_predictions[:,1]+1/4*rf_predictions[:,1]+1/4*xgb_predictions[:,1]+1/4*dnn_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.where(predictions>0.5, 1, 0)\npredictions[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n#df_submission['target'] = predictions\n#df_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dnn_predictions)\nrf_predictions[:,1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lr = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_rf = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_xgb = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_dnn = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\ndf_lr['target'] = lr_predictions[:,1]\ndf_rf['target'] = rf_predictions[:,1]\ndf_xgb['target'] = xgb_predictions[:,1]\ndf_dnn['target'] = dnn_predictions\n\ndf_lr.to_csv('res_log_reg.csv',index=False)\ndf_rf.to_csv('res_random_forest.csv',index=False)\ndf_xgb.to_csv('res_xg_boost.csv',index=False)\ndf_dnn.to_csv('res_dnn.csv',index=False)\n\n#np.savetxt('res_log_reg.csv', [lr_predictions[:,1]], delimiter=',', fmt='%f')\n#np.savetxt('res_random_forest.csv', [rf_predictions[:,1]], delimiter=',', fmt='%f')\n#np.savetxt('res_xg_boost.csv', [xgb_predictions[:,1]], delimiter=',', fmt='%f')\n#np.savetxt('res_dnn.csv', [dnn_predictions], delimiter=',', fmt='%f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}