{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP processing of twitter tweets to detect disaster situations using scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing code found online and then adapt and optimise it for the kaggle twitter dataset <br>\n",
    "For **logistic regression** and **SVM** start-up code is https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a <br>\n",
    "For hyperparameter tuning : https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5 <br>\n",
    "Add other estimators : https://scikit-learn.org/stable/supervised_learning.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score,roc_curve,auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,make_scorer\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data (tweets)\n",
    "df_train        = pd.read_csv(r'.\\input\\train.csv')\n",
    "df_test         = pd.read_csv(r'.\\input\\test.csv')\n",
    "perfect_pred_df = pd.read_csv(r'.\\input\\perfect_submission.csv')\n",
    "# import test set target\n",
    "perfect_pred    = perfect_pred_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN DATA SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleansing of train and data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    '''\n",
    "    input: string to be cleaned\n",
    "    output: cleaned string\n",
    "    '''\n",
    "    text = text.lower() # make everything lower case\n",
    "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', text) #remove url\n",
    "    text = re.compile(r'@\\S+').sub(r'', text) #remove url\n",
    "    text = re.compile(r'#').sub(r'', text) #remove #\n",
    "#    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)) #remove punctuation\n",
    "#    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS]) #remove stop words\n",
    "#    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the cleaning on text\n",
    "df_train['text_processed'] = df_train['text'].apply(text_cleaning)\n",
    "df_test['text_processed']  = df_test['text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   just happened a terrible car crash\n",
       "1    heard about earthquake is different cities, st...\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.text_processed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make train and test set inputs (X) and targets (y) into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_clean = df_train['text_processed'].tolist()\n",
    "tweets_test_clean  = df_test['text_processed'].tolist() \n",
    "\n",
    "target_train       = df_train['target'].tolist()\n",
    "target_test        = perfect_pred_df['target'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING DATA FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’. <br>\n",
    "We can usually remove these words without changing the semantics of a text and doing so often (but not always) improves the performance of a model. <br>\n",
    "Removing these stop words becomes a lot more useful when we start using longer word sequences as model features (see n-grams below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords. Only need to do it once\n",
    "#import nltk\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is considered to be the more crude/brute-force approach to normalization (although this doesn’t necessarily mean that it will perform worse). There’s several algorithms, but in general they all use basic rules to chop off the ends of words.\n",
    "\n",
    "NLTK has several stemming algorithm implementations. We’ll use the Porter stemmer here but you can explore all of the options with examples here: NLTK Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization works by identifying the part-of-speech of a given word and then applying more complex rules to transform the word into its true root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wordnet. Only need to do it once\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic processing uses only single word features in the model, which is called 1-grams or unigrams. <br> \n",
    "We can potentially add more predictive power to the model by adding two or three (or more) word sequences (bigrams or trigrams) as well.<br> \n",
    "For instance the tweet \"just happened a terrible car crash\" is transformed into following sets:<br>\n",
    "1-grams: {'just'                     , 'happened'                , 'a'              , 'terrible'     , 'car'      , 'crash'} <br>\n",
    "2-grams: {'just happened'            , 'happened a'              , 'a terrible'     , 'terrible car' , 'car crash'}<br> \n",
    "3-grams: {'just happened a'          , 'happened a terrible'     , 'a terrible car' , 'terrible car crash'}<br> \n",
    "4-grams: {'just happened a terrible' , 'happened a terrible car' , 'a terrible car crash'}<br> \n",
    "\n",
    "\n",
    "For example, if a movie review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review.\n",
    "\n",
    "The scikit-learn library makes this really easy to play around with. Just use the ngram argument in the function vectorize below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are text and for this data to make sense to our machine learning algorithm we’ll need to convert each tweet to a numeric representation, which we call vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vectorization\n",
    "The simplest form of vectorization is to create one very large matrix with one column for every unique word in your corpus (where the corpus is all tweets in our case). <br> \n",
    "Then we transform each tweet into one row containing 0s and 1s, where 1 means that the word in the corpus corresponding to that column appears in that tweet. <br> \n",
    "That being said, each row of the matrix will be very sparse (mostly zeros). <br> \n",
    "This process is also known as one hot encoding. <br> \n",
    "To use binary encoding, set binary parameter in the vectorize function to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simply noting whether a word appears in the tweet or not, we can include the number of times a given word appears. This can give our sentiment classifier a lot more predictive power. <br> \n",
    "For example, if a movie reviewer says ‘amazing’ or ‘terrible’ multiple times in a review it is considerably more probable that the review is positive or negative, respectively. <br> \n",
    "To use word count encoding, set binary parameter in the vectorize function to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text_train,text_test, binary_or_count=True, ngram=(1,1)):\n",
    "    vectorizer = CountVectorizer(binary=binary_or_count, ngram_range=ngram)\n",
    "    vectorizer.fit(text_train)\n",
    "    X = vectorizer.transform(text_train)\n",
    "    X_test = vectorizer.transform(text_test)\n",
    "    \n",
    "    return(X,X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency) for each word, which is a weighting factor that we can use in place of binary or word count representations. <br>\n",
    "\n",
    "There are several ways to do tf-idf transformation but in a nutshell, tf-idf aims to represent the number of times a given word appears in a document (a tweet in our case) relative to the number of documents in the corpus that the word appears in — where words that appear in many documents have a value closer to zero and words that appear in less documents have values closer to 1. <br>\n",
    "It can be intuitively understood as: <br>\n",
    "* A word that would appear many times in a tweet but few times over all tweets would carry a lot meaning. <br>\n",
    "* A word that would appear many times in a tweet and many times over all tweets would carry lot less meaning. <br>\n",
    "\n",
    "\n",
    "TF-IDF is done by function TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class defines the kind of pre-processing to do <br>\n",
    "it is broken down in 3 fields: <br>\n",
    "1. stop_word. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'keep'    : keep stop words<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'remove'  : remove stop words<br>\n",
    "<br>\n",
    "2. stem_lem <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'none'    : apply neither stemming not lemming <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'stem'    : apply stemming <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'lem'     : apply lemming <br>\n",
    "<br>\n",
    "3. ngram <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;range of n-gram to compute\n",
    "4. vector <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'binary'  : apply binary vectorization <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'count'   : apply words count vectorization <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;'tf_idf'  : apply tf-idf vectorization <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_pre_preproc:\n",
    "  def __init__(self, stop_word, stem_lem, ngram, vector):\n",
    "    self.stop_word = stop_word\n",
    "    self.stem_lem  = stem_lem\n",
    "    self.ngram     = ngram\n",
    "    self.vector    = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply pre-processing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs pre-processing according to the fields of the class text_pre_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pre_processing(train_text, test_text, prep, verbose=0):\n",
    "    '''\n",
    "    input: train and test texts to be processed and pre_processing attributes\n",
    "    output: vector of pre_processed train and test texts\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('text pre-processing parameters:')\n",
    "        \n",
    "    if   prep.stop_word == 'keep':\n",
    "        if verbose:\n",
    "            print('    keep stop words')\n",
    "        train_stop_word, test_stop_word = (train_text                    , test_text)\n",
    "    elif prep.stop_word =='remove':\n",
    "        if verbose:\n",
    "            print('    remove stop words')\n",
    "        train_stop_word, test_stop_word = (remove_stop_words(train_text) , remove_stop_words(test_text))\n",
    "    else:\n",
    "        raise NameError('stop_word field from class text_pre_preproc not correctly set')\n",
    "        \n",
    "    if   prep.stem_lem == 'stem':\n",
    "        if verbose:\n",
    "            print('    do stemming')\n",
    "        train_stem_lem, test_stem_lem = (get_stemmed_text(train_stop_word)    , get_stemmed_text(test_stop_word))\n",
    "    elif prep.stem_lem == 'lem':\n",
    "        if verbose:\n",
    "            print('    do lemmatization')\n",
    "        train_stem_lem, test_stem_lem = (get_lemmatized_text(train_stop_word) , get_lemmatized_text(test_stop_word))\n",
    "    elif prep.stem_lem == 'none':\n",
    "        if verbose:\n",
    "            print('    no stemming or lemmatization')\n",
    "        # do nothing\n",
    "        train_stem_lem, test_stem_lem = (train_stop_word , test_stop_word)\n",
    "    else:\n",
    "        raise NameError('stem_lem field from class text_pre_preproc not correctly set')\n",
    "            \n",
    "    if   prep.vector == 'binary':\n",
    "        if verbose:\n",
    "            print('    binary vectorization with', prep.ngram[0], 'to', prep.ngram[1], \"n-grams\")\n",
    "        train_vector, test_vector = vectorize(train_stem_lem, test_stem_lem, binary_or_count=True, ngram=prep.ngram)\n",
    "    elif prep.vector == 'count':\n",
    "        if verbose:\n",
    "            print('    word count vectorization with', prep.ngram[0], 'to', prep.ngram[1], \"n-grams\")\n",
    "        train_vector, test_vector = vectorize(train_stem_lem, test_stem_lem, binary_or_count=False, ngram=prep.ngram)\n",
    "    elif prep.vector == 'tf_idf':\n",
    "        if verbose:\n",
    "            print('    tf-idf vectorization with', prep.ngram[0], 'to', prep.ngram[1], \"n-grams\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=prep.ngram)\n",
    "        tfidf_vectorizer.fit(train_stem_lem)        \n",
    "        train_vector, test_vector = (tfidf_vectorizer.transform(train_stem_lem) , tfidf_vectorizer.transform(test_stem_lem))\n",
    "    else:\n",
    "        raise NameError('vector field from class text_pre_preproc not correctly set')\n",
    "    \n",
    "    \n",
    "    return (train_vector, test_vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pre_processing to do per algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_list = [ \n",
    "                text_pre_preproc('keep','none',(1,1),'binary'),\n",
    "                text_pre_preproc('remove','none',(1,1),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,1),'binary'),\n",
    "                text_pre_preproc('remove','stem',(1,1),'binary'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,1),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,1),'binary'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,2),'binary'),\n",
    "                text_pre_preproc('remove','none',(1,2),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,2),'binary'),\n",
    "                text_pre_preproc('remove','stem',(1,2),'binary'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,2),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'binary'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,3),'binary'),\n",
    "                text_pre_preproc('remove','none',(1,3),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,3),'binary'),\n",
    "                text_pre_preproc('remove','stem',(1,3),'binary'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,3),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','none',(2,3),'binary'),\n",
    "                text_pre_preproc('remove','none',(2,3),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(2,3),'binary'),\n",
    "                text_pre_preproc('remove','stem',(2,3),'binary'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(2,3),'binary'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'binary'),\n",
    "\n",
    "                text_pre_preproc('keep','none',(1,1),'count'),\n",
    "                text_pre_preproc('remove','none',(1,1),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,1),'count'),\n",
    "                text_pre_preproc('remove','stem',(1,1),'count'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,1),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,1),'count'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,2),'count'),\n",
    "                text_pre_preproc('remove','none',(1,2),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,2),'count'),\n",
    "                text_pre_preproc('remove','stem',(1,2),'count'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,2),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'count'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,3),'count'),\n",
    "                text_pre_preproc('remove','none',(1,3),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,3),'count'),\n",
    "                text_pre_preproc('remove','stem',(1,3),'count'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,3),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','none',(2,3),'count'),\n",
    "                text_pre_preproc('remove','none',(2,3),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(2,3),'count'),\n",
    "                text_pre_preproc('remove','stem',(2,3),'count'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(2,3),'count'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'count'),\n",
    "\n",
    "                text_pre_preproc('keep','none',(1,1),'tf_idf'),\n",
    "                text_pre_preproc('remove','none',(1,1),'tf_idf'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,1),'tf_idf'),\n",
    "                text_pre_preproc('remove','stem',(1,1),'tf_idf'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,1),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(1,1),'tf_idf'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,2),'tf_idf'),\n",
    "                text_pre_preproc('remove','none',(1,2),'tf_idf'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,2),'tf_idf'),\n",
    "                text_pre_preproc('remove','stem',(1,2),'tf_idf'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,2),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'tf_idf'),\n",
    "     \n",
    "                text_pre_preproc('keep','none',(1,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','none',(1,3),'tf_idf'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(1,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','stem',(1,3),'tf_idf'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(1,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'tf_idf'),\n",
    "\n",
    "                text_pre_preproc('keep','none',(2,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','none',(2,3),'tf_idf'),\n",
    "\n",
    "                text_pre_preproc('keep','stem',(2,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','stem',(2,3),'tf_idf'),\n",
    "    \n",
    "                text_pre_preproc('keep','lem',(2,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'tf_idf'),\n",
    "    \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_list_rf = [     \n",
    "                text_pre_preproc('remove','lem',(1,1),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'binary'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'binary'),\n",
    "                text_pre_preproc('remove','lem',(1,1),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'count'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'count'),\n",
    "                text_pre_preproc('remove','lem',(1,1),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(1,2),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(1,3),'tf_idf'),\n",
    "                text_pre_preproc('remove','lem',(2,3),'tf_idf'),    \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_algo_dic = {'lr':prep_list, 'svm':prep_list, 'rf':prep_list_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_hyper_param(prep_list_algo , algo):\n",
    "    best_hyper_param , best_accuracy, best_pred_soft , best_pred_hard = ([],[],[],[])\n",
    "    prep_list = prep_algo_dic[algo]\n",
    "    \n",
    "    max_accuracy = -1\n",
    "    for idx,prep in enumerate(prep_list):\n",
    "        X,X_test = text_pre_processing(tweets_train_clean, tweets_test_clean, prep, verbose=1)\n",
    "        \n",
    "        if algo == 'lr':\n",
    "            pipe, param_grid = logistic_regression_hyper_param()\n",
    "        elif algo == 'svm':\n",
    "            pipe, param_grid = SVM_hyper_param()\n",
    "        elif algo == 'rf':\n",
    "            pipe, param_grid = random_forest_hyper_param()\n",
    "        else:\n",
    "           raise NameError('incorrect algorithm name') \n",
    "            \n",
    "        hyper_param , accuracy, pred_soft , pred_hard = hyper_param_tuning(X, target_train, X_test, target_test, pipe, param_grid)\n",
    "\n",
    "        if accuracy >= max_accuracy:\n",
    "            max_accuracy = accuracy \n",
    "            best_hyper_param , best_accuracy, best_pred_soft , best_pred_hard         = (hyper_param , accuracy, pred_soft , pred_hard)\n",
    "            best_prep = prep_list[idx]\n",
    "        print()\n",
    "        \n",
    "    return(best_hyper_param , best_accuracy, best_pred_soft , best_pred_hard, best_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, pred_soft):\n",
    "    preds = pred_soft[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='GridSearchCV (ROC_AUV (area) = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    #plt.savefig('Log_ROC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_tuning(X, y, X_test, y_test, pipe, param_grid):\n",
    "    # Create grid search object\n",
    "    clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=False, n_jobs=-1)\n",
    "\n",
    "    # Fit on data\n",
    "    best_clf         = clf.fit(X, y)\n",
    "    \n",
    "    best_hyper_param = best_clf.best_estimator_.get_params()['classifier']\n",
    "    accuracy         = best_clf.score(X_test, y_test)\n",
    "    # some models (e.g. clustering) have no soft predictions, only hard predictions\n",
    "    try:\n",
    "        pred_soft    = best_clf.predict_proba(X_test)\n",
    "    except:\n",
    "        pred_soft    = []\n",
    "    pred_hard        = best_clf.predict(X_test)\n",
    "    \n",
    "#    print('Hyperparameter tuning leads to : ' , best_clf.best_estimator_.get_params()['classifier'])\n",
    "    print('Model accuracy is', best_clf.score(X_test, y_test))\n",
    "            \n",
    "    return(best_hyper_param , accuracy, pred_soft , pred_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_hyper_param():\n",
    "    pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "    # Create param grid.\n",
    "    param_grid = [\n",
    "                    {'classifier' : [LogisticRegression()],\n",
    "                     'classifier__penalty' : ['l1', 'l2'],\n",
    "#                     'classifier__max_iter' : [50, 100, 200, 500, 1000],\n",
    "                    'classifier__C' : np.logspace(-4, 0, 40),\n",
    "                    'classifier__solver' : ['liblinear']}\n",
    "                ]\n",
    "    \n",
    "    return(pipe,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_hyper_param():\n",
    "    pipe = Pipeline([('classifier' , LinearSVC())])\n",
    "\n",
    "    # Create param grid.\n",
    "    param_grid = [\n",
    "                    {'classifier' : [LinearSVC()],\n",
    "#                     'classifier__max_iter' : [50, 100, 200, 500, 1000],\n",
    "                    'classifier__C' : np.logspace(-4, 0, 40),\n",
    "                    'classifier__loss' : ['hinge','squared_hinge']\n",
    "                    }\n",
    "                ]\n",
    "    \n",
    "    return(pipe,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_hyper_param():\n",
    "    pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "\n",
    "    # Create param grid.\n",
    "    param_grid = [\n",
    "                    {'classifier' : [RandomForestClassifier()],\n",
    "                    'classifier__n_estimators' : list(range(10,101,10)),\n",
    "                    'classifier__max_features' : list(range(6,32,5))}\n",
    "                ]\n",
    "    \n",
    "    return(pipe,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7876187557462457\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7796506282562059\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7891510879558689\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.774134232301563\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.8004903463070794\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7971192154459087\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7995709469813056\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7906834201654919\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7931351517008888\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7974256818878332\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8038614771682501\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7989580140974564\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7949739503524365\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7931351517008888\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7949739503524365\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.8026356114005516\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7330677290836654\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7318418633159669\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7450199203187251\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7330677290836654\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7407293901317805\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7321483297578915\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7995709469813056\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7940545510266626\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7851670242108489\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7897640208397181\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7983450812136071\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7992644805393809\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8020226785167024\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8001838798651547\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7891510879558689\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7916028194912657\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7980386147716825\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8029420778424763\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7986515476555317\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7989580140974564\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7943610174685872\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7906834201654919\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.8007968127490039\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7324547961998161\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7358259270609868\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7462457860864236\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7346000612932884\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7382776585963837\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7370517928286853\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7928286852589641\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7971192154459087\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7928286852589641\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7900704872816426\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7931351517008888\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7995709469813056\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8001838798651547\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7977321483297579\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8029420778424763\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7995709469813056\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.806313208703647\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7998774134232302\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7965062825620595\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.8004903463070794\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.8011032791909286\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7998774134232302\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7407293901317805\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7416487894575544\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxa19765\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 0.6769843702114618\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.6754520380018388\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.6736132393502912\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    tf-idf vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.6763714373276126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_best_hyper_param , lr_best_accuracy, lr_best_pred_soft , lr_best_pred_hard, lr_best_prep = do_hyper_param(prep_algo_dic , 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best pre_processing is : { keep , lem , (1, 2) , tf_idf }\n",
      "best model is  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "best accuracy is  0.806313208703647\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('best pre_processing is :', '{',lr_best_prep.stop_word, ',' ,lr_best_prep.stem_lem, ',' ,lr_best_prep.ngram, ',' , lr_best_prep.vector ,'}') \n",
    "print('best model is ', lr_best_hyper_param)\n",
    "print('best accuracy is ', lr_best_accuracy)\n",
    "display_results(target_test, lr_best_pred_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7946674839105118\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7949739503524365\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7836346920012258\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7931351517008888\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7781182960465829\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.8011032791909286\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7983450812136071\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7989580140974564\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7949739503524365\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7912963530493411\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7925222188170395\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8038614771682501\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7961998161201348\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7980386147716825\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.796812749003984\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7983450812136071\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.793748084584738\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.8011032791909286\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7447134538768004\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7327612626417407\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7502298498314435\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7373582592706098\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7477781182960466\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    binary vectorization with 2 to 3 n-grams\n",
      "Model accuracy is 0.7358259270609868\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.795280416794361\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7940545510266626\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7799570946981306\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7955868832362857\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7897640208397181\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 1 n-grams\n",
      "Model accuracy is 0.7983450812136071\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.795280416794361\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7983450812136071\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7949739503524365\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do stemming\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7986515476555317\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.7946674839105118\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    do lemmatization\n",
      "    word count vectorization with 1 to 2 n-grams\n",
      "Model accuracy is 0.8001838798651547\n",
      "\n",
      "text pre-processing parameters:\n",
      "    keep stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n",
      "Model accuracy is 0.7971192154459087\n",
      "\n",
      "text pre-processing parameters:\n",
      "    remove stop words\n",
      "    no stemming or lemmatization\n",
      "    word count vectorization with 1 to 3 n-grams\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-899b856f414d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvm_best_hyper_param\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msvm_best_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_best_pred_soft\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msvm_best_pred_hard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_best_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_hyper_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_algo_dic\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'svm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-ea7918254525>\u001b[0m in \u001b[0;36mdo_hyper_param\u001b[1;34m(prep_list_algo, algo)\u001b[0m\n\u001b[0;32m     16\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'incorrect algorithm name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mhyper_param\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_soft\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpred_hard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyper_param_tuning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-51c691bfde9a>\u001b[0m in \u001b[0;36mhyper_param_tuning\u001b[1;34m(X, y, X_test, y_test, pipe, param_grid)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Fit on data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mbest_clf\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbest_hyper_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svm_best_hyper_param , svm_best_accuracy, svm_best_pred_soft , svm_best_pred_hard, svm_best_prep = do_hyper_param(prep_algo_dic , 'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best pre_processing is :', '{',svm_best_prep.stop_word, ',' ,svm_best_prep.stem_lem, ',' ,svm_best_prep.ngram, ',' , svm_best_prep.vector ,'}') \n",
    "print('best model is ', svm_best_hyper_param)\n",
    "print('best accuracy is ', svm_best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_hyper_param , rf_best_accuracy, rf_best_pred_soft , rf_best_pred_hard, rf_best_prep = do_hyper_param(prep_algo_dic , 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best pre_processing is :', '{',rf_best_prep.stop_word, ',' ,rf_best_prep.stem_lem, ',' ,rf_best_prep.ngram, ',' , rf_best_prep.vector ,'}') \n",
    "print('best model is ', rf_best_hyper_param)\n",
    "print('best accuracy is ', rf_best_accuracy)\n",
    "display_results(target_test, rf_best_pred_soft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
